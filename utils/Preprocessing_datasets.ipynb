{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>  This file is for the preprocessing of the multiple datasets </h1>\n",
    "It includes : </br>\n",
    "- Resizing the images to 224x224 (TODO) </br>\n",
    "- Separate datasets into train/dev/test - 80/10/10 (TODO)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Imports and Global Variables </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import csv\n",
    "\n",
    "PATH_FIRECORSE = \"./../dataset/data/fire_corse/\"\n",
    "PATH_FIRE_SMOKE = \"./../dataset/data/FIRE-SMOKE-DATASET/\"\n",
    "PATH_FIRE_DETECTION = \"./../dataset/data/Fire-Detection-Image-Dataset/\"\n",
    "PATH_FIRE_KAGGLE = \"./../dataset/data/Fire-Kaggle/\"\n",
    "PATH_WILDFIRE = \"./../dataset/data/Wildfire-forest-fire/\"\n",
    "\n",
    "PATH_TARGET_FINAL = \"./../dataset/data_preprocessed/\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Data exploration </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'jpg': 3000}\n"
     ]
    }
   ],
   "source": [
    "#Finding the different image extensions\n",
    "extensions_dict = {}\n",
    "\n",
    "path_list = [PATH_FIRECORSE,PATH_FIRE_SMOKE,PATH_FIRE_DETECTION]\n",
    "\n",
    "for p in path_list:\n",
    "    for (path,dirs,files) in os.walk(p):\n",
    "        if files:\n",
    "            for f in files:\n",
    "                f_ext = f.split('.')[-1]\n",
    "\n",
    "                if f_ext in extensions_dict:\n",
    "                    extensions_dict[f_ext] += 1\n",
    "                else:\n",
    "                    extensions_dict[f_ext] = 1\n",
    "\n",
    "print(extensions_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Resizing images </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_crop(img, new_width=None, new_height=None):        \n",
    "    width, height = img.size   # Get dimensions\n",
    "\n",
    "    left = (width - new_width)/2\n",
    "    top = (height - new_height)/2\n",
    "    right = (width + new_width)/2\n",
    "    bottom = (height + new_height)/2\n",
    "\n",
    "    # Crop the center of the image\n",
    "    img = img.crop((left, top, right, bottom))\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the photos with max and min size, to help visualize what we are dealing with\n",
    "def find_extreme_images_paths(main_path):\n",
    "    MAXH,MINH,MAXW,MINW = [-1,None],[10000,None],[-1,None],[10000,None]\n",
    "    for (path,dirs,files) in os.walk(main_path):\n",
    "        print(path)\n",
    "        if files:\n",
    "            for f in files:\n",
    "                img = Image.open(path+'/'+f)\n",
    "                \n",
    "                if img.height > MAXH[0]:\n",
    "                    MAXH = [img.height,path+'/'+f]\n",
    "                elif img.height < MINH[0]:\n",
    "                    MINH = [img.height,path+'/'+f]\n",
    "                elif img.width > MAXW[0]:\n",
    "                    MAXW = [img.width,path+'/'+f]\n",
    "                elif img.width < MINW[0]:\n",
    "                    MINW = [img.width,path+'/'+f]\n",
    "\n",
    "    list_path = [*set([MAXH[1],MINH[1],MAXW[1],MINW[1]])]\n",
    "    return list_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_img(im_path):\n",
    "    img = Image.open(im_path)\n",
    "\n",
    "    img.thumbnail((225,225))\n",
    "\n",
    "    img = center_crop(img,224,224)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Test Cell\n",
    "# list_h,list_w = [],[]\n",
    "# extreme_images_path = find_extreme_images_paths(\"FIRE-SMOKE-DATASET\")\n",
    "# for im_path in extreme_images_path:\n",
    "#     img = Image.open(im_path)\n",
    "\n",
    "#     display(img)\n",
    "\n",
    "#     img_tr = img.thumbnail((224,224))\n",
    "\n",
    "#     display(img)\n",
    "\n",
    "#     img = center_crop(img,224,224)\n",
    "\n",
    "#     display(img)\n",
    "\n",
    "#     im = resize_img(im_path)\n",
    "#     im.save('IMAGETEST.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOBAL_IMAGE_COUNTER = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_print(current_count,size):\n",
    "    if current_count%(round(size/100)) == 0:\n",
    "        print(\"\\r\", end=\"\")\n",
    "        print(round(current_count/(size/100),1),\"% Done\",end=\"\")\n",
    "    if current_count == size:\n",
    "        print(\"\\r\", end=\"\")\n",
    "        print(\"100.0 % Done\",end=\"\")\n",
    "\n",
    "def resize_images(path_origin,path_target):\n",
    "    global GLOBAL_IMAGE_COUNTER\n",
    "    count = 0\n",
    "\n",
    "    #Create necessary directories\n",
    "    os.makedirs(path_target, exist_ok=True)\n",
    "\n",
    "    #Resize images\n",
    "    print(\"\\nStart rezise images : \"+path_origin)\n",
    "    for (path,dirs,files) in os.walk(path_origin):\n",
    "        \n",
    "        if files:\n",
    "            for f in files:\n",
    "                #Print progress\n",
    "                count += 1\n",
    "                process_print(count,len(files))\n",
    "                #Get file extension\n",
    "                f_ext = f.split('.')[-1]\n",
    "                if f_ext in ['jpg','png','JPG','jpeg'] and not '_nir.' in f:\n",
    "                    im = resize_img(path_origin+f)\n",
    "                    im.save(path_target+\"image_\"+str(GLOBAL_IMAGE_COUNTER)+\".png\")\n",
    "                    GLOBAL_IMAGE_COUNTER += 1\n",
    "\n",
    "#Ratio = (train,dev,test)\n",
    "#/!\\ path_origin and path_target MUST be different (Tips: create a temporary folder)\n",
    "def split_images(path_origin,path_target,ratio=(8,1,1)):\n",
    "    cpt = 0\n",
    "    count = 0\n",
    "    (n_train,n_dev,n_test) = ratio\n",
    "\n",
    "    #Create necessary directories\n",
    "    os.makedirs(path_target+\"train\", exist_ok=True)\n",
    "    os.makedirs(path_target+\"dev\", exist_ok=True)\n",
    "    os.makedirs(path_target+\"test\", exist_ok=True)\n",
    "\n",
    "    #Split Images\n",
    "    print(\"\\nStart split images\")\n",
    "    for (path,dirs,files) in os.walk(path_origin):\n",
    "        \n",
    "        if files:\n",
    "            for f in files:\n",
    "                #Print progress\n",
    "                count += 1\n",
    "                process_print(count,len(files))\n",
    "\n",
    "                f_ext = f.split('.')[-1]\n",
    "                if f_ext in ['jpg','png','JPG','jpeg']:\n",
    "\n",
    "                    if cpt < n_dev:\n",
    "                        save_path = path_target+\"dev/\"+f\n",
    "                    elif cpt < n_dev + n_test:\n",
    "                        save_path = path_target+\"test/\"+f\n",
    "                    elif cpt < n_dev + n_test + n_train:\n",
    "                        save_path = path_target+\"train/\"+f\n",
    "\n",
    "                    os.rename(path_origin+f,save_path)\n",
    "\n",
    "                    if cpt == n_dev + n_test + n_train -1:\n",
    "                        cpt = 0\n",
    "                    else:\n",
    "                        cpt += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate(path_dir,path_file,notation):\n",
    "    file_list = os.listdir(path_dir)\n",
    "\n",
    "    annotations = []\n",
    "    for fname in file_list:\n",
    "        annotations.append([str(fname),int(notation)])\n",
    "\n",
    "    file = open(path_file, 'a', newline ='')\n",
    "    \n",
    "    with file:\n",
    "        write = csv.writer(file)\n",
    "        write.writerows(annotations)\n",
    "    \n",
    "    file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start rezise images : ./../dataset/data/Fire-Detection-Image-Dataset/Fire_images/\n",
      "\n",
      "Start split images\n",
      "\n",
      "Start rezise images : ./../dataset/data/Fire-Detection-Image-Dataset/Normal_images/\n",
      "\n",
      "Start split images\n",
      "\n",
      "Start rezise images : ./../dataset/data/fire_corse/\n",
      "\n",
      "Start split images\n",
      "\n",
      "Start rezise images : ./../dataset/data/FIRE-SMOKE-DATASET/Test/Fire/\n",
      "100.0 % Done\n",
      "Start split images\n",
      "100.0 % Done\n",
      "Start rezise images : ./../dataset/data/FIRE-SMOKE-DATASET/Test/Neutral/\n",
      "100.0 % Done\n",
      "Start split images\n",
      "100.0 % Done\n",
      "Start rezise images : ./../dataset/data/FIRE-SMOKE-DATASET/Test/Smoke/\n",
      "100.0 % Done\n",
      "Start split images\n",
      "100.0 % Done\n",
      "Start rezise images : ./../dataset/data/FIRE-SMOKE-DATASET/Train/Fire/\n",
      "100.0 % Done\n",
      "Start split images\n",
      "100.0 % Done\n",
      "Start rezise images : ./../dataset/data/FIRE-SMOKE-DATASET/Train/Neutral/\n",
      "100.0 % Done\n",
      "Start split images\n",
      "100.0 % Done\n",
      "Start rezise images : ./../dataset/data/FIRE-SMOKE-DATASET/Train/Smoke/\n",
      "100.0 % Done\n",
      "Start split images\n",
      "100.0 % Done\n",
      "Start rezise images : ./../dataset/data/Fire-Kaggle/fire/\n",
      "100.0 % Done\n",
      "Start split images\n",
      "100.0 % Done\n",
      "Start rezise images : ./../dataset/data/Fire-Kaggle/no_fire/\n",
      "100.0 % Done\n",
      "Start split images\n",
      "100.0 % Done\n",
      "Start rezise images : ./../dataset/data/Wildfire-forest-fire/\n",
      "100.0 % Done\n",
      "Start split images\n",
      "100.0 % Done"
     ]
    }
   ],
   "source": [
    "path_annotation = PATH_TARGET_FINAL+\"annotations/annotations.csv\"\n",
    "path_tmp = PATH_TARGET_FINAL+\"Tmp_folder/\"\n",
    "path_target = PATH_TARGET_FINAL+\"images/\"\n",
    "\n",
    "os.makedirs(PATH_TARGET_FINAL+\"annotations/\", exist_ok=True)\n",
    "\n",
    "def process_folder(path_folder,path_tmp,path_target,path_annotation,annotation):\n",
    "    resize_images(path_folder,path_tmp)\n",
    "    annotate(path_tmp,path_annotation,int(annotation))\n",
    "    split_images(path_tmp,path_target)\n",
    "\n",
    "\n",
    "#Fire-Detection\n",
    "#/!\\ You need to create and manually put the raw images in Fire_images and Normal_images\n",
    "path_dir = PATH_FIRE_DETECTION\n",
    "process_folder(path_dir+\"Fire_images/\",path_tmp,path_target,path_annotation,1)\n",
    "process_folder(path_dir+\"Normal_images/\",path_tmp,path_target,path_annotation,0)\n",
    "\n",
    "#FireCorse\n",
    "path_dir = PATH_FIRECORSE\n",
    "process_folder(path_dir,path_tmp,path_target,path_annotation,1)\n",
    "\n",
    "#FIRE-SMOKE\n",
    "path_dir = PATH_FIRE_SMOKE\n",
    "process_folder(path_dir+\"Test/Fire/\",path_tmp,path_target,path_annotation,1)\n",
    "process_folder(path_dir+\"Test/Neutral/\",path_tmp,path_target,path_annotation,0)\n",
    "process_folder(path_dir+\"Test/Smoke/\",path_tmp,path_target,path_annotation,0)\n",
    "process_folder(path_dir+\"Train/Fire/\",path_tmp,path_target,path_annotation,1)\n",
    "process_folder(path_dir+\"Train/Neutral/\",path_tmp,path_target,path_annotation,0)\n",
    "process_folder(path_dir+\"Train/Smoke/\",path_tmp,path_target,path_annotation,0)\n",
    "\n",
    "#Fire KAGGLE \n",
    "path_dir = PATH_FIRE_KAGGLE\n",
    "process_folder(path_dir+\"fire/\",path_tmp,path_target,path_annotation,1)\n",
    "process_folder(path_dir+\"no_fire/\",path_tmp,path_target,path_annotation,0)\n",
    "\n",
    "#Wildfire\n",
    "path_dir = PATH_WILDFIRE\n",
    "process_folder(path_dir,path_tmp,path_target,path_annotation,1)\n",
    "\n",
    "os.rmdir(path_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_annotation = PATH_TARGET_FINAL+\"annotations/\"\n",
    "path_data = PATH_TARGET_FINAL+\"images/\"\n",
    "\n",
    "dir_list = [\"dev\",\"train\",\"test\"]\n",
    "ids = {}\n",
    "reversed_ids = {}\n",
    "header = [\"name\",\"label\"]\n",
    "annotations_different_csv = {\"dev\":[],\"test\":[],\"train\":[]}\n",
    "\n",
    "for dir in dir_list:\n",
    "    (path,dirs,files) = list(os.walk(path_data+dir))[0]\n",
    "    ids[dir] = files\n",
    "\n",
    "# Création du dictionnaire inversé : nom de l'image -> nom du dossier\n",
    "for dir in ids:\n",
    "    list_name = ids[dir]\n",
    "    for name in list_name:\n",
    "        reversed_ids[name] = dir\n",
    "\n",
    "# Lecture du fichier de toutes les annotations\n",
    "file = open(path_annotation+\"annotations.csv\", 'r', newline ='')   \n",
    "with file:\n",
    "    for row in file:\n",
    "        row_list = row.split(',')\n",
    "        im_name = row_list[0]\n",
    "        row_list[1]=int(row_list[1])\n",
    "        dir = reversed_ids[im_name]\n",
    "        annotations_different_csv[dir].append(row_list)\n",
    "\n",
    "file.close()\n",
    "\n",
    "for dir in dir_list:\n",
    "    file = open(path_annotation+\"annotations_\"+dir+\".csv\", \"w+\",newline=\"\")\n",
    "    \n",
    "    with file:\n",
    "        write = csv.writer(file)\n",
    "        write.writerows(annotations_different_csv[dir])\n",
    "    \n",
    "    file.close()\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
